# ç¬¬5ç«  LangChain ä¸ LangGraph æ ¸å¿ƒæ¦‚å¿µ:ä»ä¼ ç»Ÿå‡½æ•°åˆ°çŠ¶æ€å›¾çš„èŒƒå¼è½¬å˜

## ç« èŠ‚ç›®æ ‡

1. ç†è§£ LangChain çš„é“¾å¼è°ƒç”¨æœºåˆ¶,æŒæ¡ Prompt + LLM + OutputParser çš„ç»„åˆæ¨¡å¼
2. å­¦ä¼š LangGraph çš„çŠ¶æ€å›¾è®¾è®¡å“²å­¦,ç”¨èŠ‚ç‚¹+è¾¹æ›¿ä»£å¤æ‚çš„ if-else é€»è¾‘
3. æŒæ¡ TypedDict å®šä¹‰çŠ¶æ€çš„æœ€ä½³å®è·µ,å®ç°ç±»å‹å®‰å…¨çš„æ•°æ®æµè½¬
4. é€šè¿‡å¯¹æ¯”ä¼ ç»Ÿä»£ç ä¸ LangGraph å®ç°,ç†è§£å…¶åœ¨å¤æ‚ä¸šåŠ¡åœºæ™¯ä¸‹çš„ä¼˜åŠ¿

## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ LangChain/LangGraph

### 1.1 ä¼ ç»Ÿ LLM è°ƒç”¨çš„ç—›ç‚¹

**åé¢æ¡ˆä¾‹(ç›´æ¥è°ƒç”¨ OpenAI API):**
```python
import openai

def ask_llm(question):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# é—®é¢˜1: æ¯æ¬¡éƒ½è¦æ‰‹åŠ¨æ„å»º messages æ ¼å¼
# é—®é¢˜2: æ— æ³•å¤ç”¨æç¤ºè¯æ¨¡æ¿
# é—®é¢˜3: å¤šè½®å¯¹è¯éœ€è¦æ‰‹åŠ¨ç®¡ç†å†å²è®°å½•
# é—®é¢˜4: é”™è¯¯å¤„ç†ã€é‡è¯•é€»è¾‘éœ€è¦è‡ªå·±å®ç°
```

**LangChain è§£å†³æ–¹æ¡ˆ:**
```python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# æç¤ºè¯æ¨¡æ¿åŒ–
prompt = ChatPromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ª{role},è¯·å›ç­”:{question}")
llm = ChatOpenAI(model="gpt-4")

# é“¾å¼è°ƒç”¨
chain = prompt | llm  # LCEL (LangChain Expression Language)
response = chain.invoke({"role": "æ•°æ®åˆ†æå¸ˆ", "question": "å¦‚ä½•ä¼˜åŒ–SQLæŸ¥è¯¢?"})
```

**ä¸‰å¤§ä¼˜åŠ¿:**
1. **æ¨¡æ¿åŒ–**: æç¤ºè¯å¯å¤ç”¨,å˜é‡è‡ªåŠ¨æ›¿æ¢
2. **é“¾å¼ç»„åˆ**: é€šè¿‡ `|` ç®¡é“ç¬¦ä¸²è”å¤šä¸ªç»„ä»¶
3. **ç»Ÿä¸€æ¥å£**: ä¸åŒ LLM æä¾›å•†ä½¿ç”¨ç›¸åŒçš„ API

### 1.2 ä¼ ç»Ÿæµç¨‹æ§åˆ¶çš„å±€é™æ€§

**åœºæ™¯:** Text2SQL æ™ºèƒ½ä½“éœ€è¦æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œ:
1. æ£€ç´¢ç›¸å…³è¡¨ç»“æ„
2. ç”Ÿæˆ SQL
3. æ‰§è¡Œ SQL
4. æ€»ç»“ç»“æœ
5. ç”Ÿæˆå›¾è¡¨

**ä¼ ç»Ÿå†™æ³•(if-else åµŒå¥—åœ°ç‹±):**
```python
async def text2sql_pipeline(user_query):
    # æ­¥éª¤1: æ£€ç´¢è¡¨ç»“æ„
    db_info = await get_table_schema(user_query)
    if not db_info:
        return "æœªæ‰¾åˆ°ç›¸å…³è¡¨"

    # æ­¥éª¤2: ç”Ÿæˆ SQL
    sql = await generate_sql(user_query, db_info)
    if not sql:
        return "SQLç”Ÿæˆå¤±è´¥"

    # æ­¥éª¤3: æ‰§è¡Œ SQL
    result = await execute_sql(sql)
    if not result.success:
        # ä¿®æ­£ SQL å¹¶é‡è¯•
        corrected_sql = await correct_sql(sql, result.error)
        result = await execute_sql(corrected_sql)
        if not result.success:
            return "SQLæ‰§è¡Œå¤±è´¥"

    # æ­¥éª¤4: æ€»ç»“
    summary = await summarize(result.data)

    # æ­¥éª¤5: ç”Ÿæˆå›¾è¡¨
    if is_chart_needed(user_query):
        chart = await generate_chart(result.data)
        return {"summary": summary, "chart": chart}
    else:
        return {"summary": summary}

# é—®é¢˜:
# 1. ä»£ç åµŒå¥—å±‚çº§æ·±,éš¾ä»¥ç»´æŠ¤
# 2. é”™è¯¯å¤„ç†é€»è¾‘åˆ†æ•£
# 3. éš¾ä»¥å¯è§†åŒ–æ•´ä¸ªæµç¨‹
# 4. éš¾ä»¥åŠ¨æ€è°ƒæ•´æ­¥éª¤é¡ºåº
```

**LangGraph è§£å†³æ–¹æ¡ˆ(çŠ¶æ€å›¾):**
```python
from langgraph.graph import StateGraph, END

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    user_query: str
    db_info: Optional[Dict]
    generated_sql: Optional[str]
    execution_result: Optional[ExecutionResult]
    report_summary: Optional[str]
    chart_url: Optional[str]

# åˆ›å»ºçŠ¶æ€å›¾
graph = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("schema_inspector", get_table_schema)
graph.add_node("sql_generator", generate_sql)
graph.add_node("sql_executor", execute_sql)
graph.add_node("summarize", summarize)
graph.add_node("data_render", generate_chart)

# æ·»åŠ è¾¹(å®šä¹‰æ‰§è¡Œé¡ºåº)
graph.set_entry_point("schema_inspector")
graph.add_edge("schema_inspector", "sql_generator")
graph.add_edge("sql_generator", "sql_executor")
graph.add_edge("sql_executor", "summarize")
graph.add_conditional_edges(
    "summarize",
    lambda state: "data_render" if state.get("chart_url") else END
)

graph_compiled = graph.compile()

# æ‰§è¡Œ
result = await graph_compiled.ainvoke({"user_query": "ç»Ÿè®¡è®¢å•æ•°æ®"})
```

**ä¼˜åŠ¿:**
1. **å¯è§†åŒ–**: å¯ä»¥ç”Ÿæˆæµç¨‹å›¾(Mermaid)
2. **æ¨¡å—åŒ–**: æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹æµ‹è¯•
3. **çµæ´»æ€§**: é€šè¿‡æ¡ä»¶è¾¹åŠ¨æ€è·¯ç”±
4. **å¯è§‚æµ‹**: å†…ç½®çŠ¶æ€è¿½è¸ª

---

## äºŒã€LangChain æ ¸å¿ƒæ¦‚å¿µ

### 2.1 Prompt Template(æç¤ºè¯æ¨¡æ¿)

**åŸºæœ¬ç”¨æ³•:**
```python
from langchain.prompts import ChatPromptTemplate

# æ–¹å¼1: ç®€å•å­—ç¬¦ä¸²æ¨¡æ¿
prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ª{role},è¯·å›ç­”:{question}"
)

# æ–¹å¼2: å¤šè½®å¯¹è¯æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}"),
    ("human", "{question}"),
    ("ai", "æˆ‘ç†è§£ä½ çš„é—®é¢˜,è®©æˆ‘åˆ†æä¸€ä¸‹..."),
    ("human", "è¯·ç»§ç»­")
])

# è°ƒç”¨
formatted = prompt.format(role="DBA", question="å¦‚ä½•ä¼˜åŒ–ç´¢å¼•?")
print(formatted)
```

**é¡¹ç›®å®æˆ˜ç¤ºä¾‹(`agent/text2sql/sql/generator.py:17`):**
```python
from langchain.prompts import ChatPromptTemplate

def sql_generate(state):
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åº“ç®¡ç†å‘˜(DBA),ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„æ•°æ®åº“ç»“æ„ã€è¡¨å…³ç³»ä»¥åŠç”¨æˆ·éœ€æ±‚,ç”Ÿæˆä¼˜åŒ–çš„MYSQL SQLæŸ¥è¯¢è¯­å¥ã€‚

        ## ä»»åŠ¡
          - æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆä¸€æ¡ä¼˜åŒ–çš„SQLè¯­å¥ã€‚
          - ä»å›¾è¡¨å®šä¹‰ä¸­é€‰æ‹©æœ€åˆé€‚çš„å›¾è¡¨ç±»å‹ã€‚

        ## çº¦æŸæ¡ä»¶
         1. ä½ å¿…é¡»ä»…ç”Ÿæˆä¸€æ¡åˆæ³•ã€å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥ã€‚
         2. å¿…é¡»ç›´æ¥ä½¿ç”¨æ‰€æä¾›çš„è¡¨ç»“æ„å’Œè¡¨å…³ç³»æ¥ç”ŸæˆSQLè¯­å¥ã€‚
         3. ä½¿ç”¨é€‚å½“çš„SQLå­å¥(JOINã€WHEREã€GROUP BYã€HAVINGç­‰)ã€‚
         4. å¦‚æœç”¨æˆ·é—®é¢˜æ¨¡ç³Š,è¯·è¿”å›:`NULL`

       ## æä¾›çš„ä¿¡æ¯
        - è¡¨ç»“æ„:{db_schema}
        - è¡¨å…³ç³»:{table_relationship}
        - ç”¨æˆ·æé—®:{user_query}
        - å½“å‰æ—¶é—´:{current_time}

        ## è¾“å‡ºæ ¼å¼
        {{
            "sql_query": "ç”Ÿæˆçš„SQLè¯­å¥",
            "chart_type": "æ¨èçš„å›¾è¡¨ç±»å‹"
        }}
    """
    )

    chain = prompt | llm

    response = chain.invoke({
        "db_schema": state["db_info"],
        "user_query": state["user_query"],
        "table_relationship": state.get("table_relationship", []),
        "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    })

    # è§£æ JSON å“åº”
    clean_json_str = response.content.strip().removeprefix("```json").strip().removesuffix("```").strip()
    result = json.loads(clean_json_str)
    state["generated_sql"] = result["sql_query"]
    state["chart_type"] = result["chart_type"]

    return state
```

**å…³é”®è®¾è®¡ç‚¹:**
1. **å¤šå±‚æ¬¡çº¦æŸ**: ä»»åŠ¡ â†’ çº¦æŸæ¡ä»¶ â†’ è¾“å‡ºæ ¼å¼,å¼•å¯¼ LLM ç”Ÿæˆç¬¦åˆè¦æ±‚çš„ç»“æœ
2. **ä¸Šä¸‹æ–‡æ³¨å…¥**: å°† `db_schema`ã€`user_query` ç­‰åŠ¨æ€ä¿¡æ¯ä¼ å…¥
3. **ç»“æ„åŒ–è¾“å‡º**: å¼ºåˆ¶è¦æ±‚ JSON æ ¼å¼,æ–¹ä¾¿åç»­è§£æ

### 2.2 LLM è°ƒç”¨ä¸é…ç½®

**ç»Ÿä¸€ LLM é…ç½®(`common/llm_util.py`):**
```python
import os
from langchain_openai import ChatOpenAI

def get_llm():
    """è·å–ç»Ÿä¸€é…ç½®çš„ LLM å®ä¾‹"""
    return ChatOpenAI(
        model=os.getenv("MODEL_NAME", "qwen-plus"),
        temperature=float(os.getenv("MODEL_TEMPERATURE", 0.75)),
        base_url=os.getenv("MODEL_BASE_URL"),
        api_key=os.getenv("MODEL_API_KEY"),
        max_tokens=int(os.getenv("MAX_TOKENS", 100000)),
        top_p=float(os.getenv("TOP_P", 0.8)),
        timeout=float(os.getenv("REQUEST_TIMEOUT", 300.0)),
        max_retries=int(os.getenv("MAX_RETRIES", 3)),
        streaming=True,  # æµå¼è¾“å‡º
    )
```

**ä¸ºä»€ä¹ˆé›†ä¸­é…ç½®:**
1. **ç¯å¢ƒéš”ç¦»**: ä¸åŒç¯å¢ƒ(dev/test/pro)ä½¿ç”¨ä¸åŒçš„æ¨¡å‹é…ç½®
2. **ç»Ÿä¸€ç®¡ç†**: ä¿®æ”¹æ¨¡å‹å‚æ•°åªéœ€æ”¹ä¸€å¤„
3. **æˆæœ¬æ§åˆ¶**: å¯ä»¥ç»Ÿä¸€è®¾ç½® token é™åˆ¶

**å¸¸ç”¨å‚æ•°è§£æ:**
- `temperature`: æ¸©åº¦å‚æ•°(0-1)
  - 0 = ç¡®å®šæ€§è¾“å‡º(æ¯æ¬¡ç»“æœç›¸åŒ)
  - 0.7 = åˆ›æ„ä¸å‡†ç¡®æ€§å¹³è¡¡
  - 1 = æœ€å¤§éšæœºæ€§
- `max_tokens`: æœ€å¤§è¾“å‡º token æ•°
- `top_p`: æ ¸é‡‡æ ·å‚æ•°(0.8 è¡¨ç¤ºä»æ¦‚ç‡ç´¯ç§¯åˆ° 80% çš„è¯ä¸­é‡‡æ ·)
- `streaming`: æ˜¯å¦æµå¼è¿”å›(True æ—¶å¯å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹)

### 2.3 LCEL(LangChain Expression Language) é“¾å¼è°ƒç”¨

**ç®¡é“ç¬¦ç»„åˆ:**
```python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("æ€»ç»“è¿™æ®µæ–‡å­—:{text}")
llm = ChatOpenAI()
output_parser = StrOutputParser()

# é“¾å¼ç»„åˆ
chain = prompt | llm | output_parser

# ç­‰ä»·äº:
# step1 = prompt.format(text=input_text)
# step2 = llm.invoke(step1)
# step3 = output_parser.parse(step2)

result = chain.invoke({"text": "LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶..."})
```

**å¤æ‚é“¾ç¤ºä¾‹:**
```python
from langchain.schema.runnable import RunnablePassthrough

# å¤šåˆ†æ”¯é“¾
chain = (
    {"context": retriever, "question": RunnablePassthrough()}  # å¹¶è¡Œæ‰§è¡Œ
    | prompt
    | llm
    | StrOutputParser()
)

# æ¡ä»¶é“¾
from langchain.schema.runnable import RunnableBranch

branch = RunnableBranch(
    (lambda x: "SQL" in x["query"], sql_chain),
    (lambda x: "å›¾è¡¨" in x["query"], chart_chain),
    default_chain
)
```

---

## ä¸‰ã€LangGraph æ ¸å¿ƒæ¦‚å¿µ

### 3.1 State(çŠ¶æ€):æ•°æ®çš„ç»Ÿä¸€å®¹å™¨

**ä¸ºä»€ä¹ˆéœ€è¦ State:**

ä¼ ç»Ÿæ–¹å¼:
```python
# æ¯ä¸ªå‡½æ•°éƒ½è¦è¿”å›æ‰€æœ‰ä¸­é—´ç»“æœ
def step1(query):
    db_info = get_db_info(query)
    return {"query": query, "db_info": db_info}

def step2(data):
    sql = generate_sql(data["query"], data["db_info"])
    return {"query": data["query"], "db_info": data["db_info"], "sql": sql}

# å‚æ•°è¶Šæ¥è¶Šå¤š,å®¹æ˜“é—æ¼
```

LangGraph State:
```python
from typing import TypedDict, Optional

class AgentState(TypedDict):
    """æ‰€æœ‰æ­¥éª¤å…±äº«çš„çŠ¶æ€"""
    user_query: str
    db_info: Optional[Dict]
    generated_sql: Optional[str]
    execution_result: Optional[ExecutionResult]

# æ¯ä¸ªèŠ‚ç‚¹åªéœ€è¦å…³æ³¨è‡ªå·±éœ€è¦çš„å­—æ®µ
def step1(state: AgentState) -> AgentState:
    state["db_info"] = get_db_info(state["user_query"])
    return state  # è‡ªåŠ¨åˆå¹¶åˆ°å…¨å±€çŠ¶æ€

def step2(state: AgentState) -> AgentState:
    state["generated_sql"] = generate_sql(state["user_query"], state["db_info"])
    return state  # å¯ä»¥è®¿é—®å‰é¢æ­¥éª¤çš„ç»“æœ
```

**é¡¹ç›®å®æˆ˜(`agent/text2sql/state/agent_state.py:48`):**
```python
from typing import TypedDict, Optional, Dict, List, Any
from pydantic import BaseModel, Field

class ExecutionResult(BaseModel):
    """SQLæ‰§è¡Œç»“æœ"""
    success: bool
    data: Optional[List[Dict[str, Any]]] = None
    error: Optional[str] = None

class AgentState(TypedDict):
    """Text2SQL æ™ºèƒ½ä½“çš„çŠ¶æ€å®šä¹‰"""
    user_query: str  # ç”¨æˆ·é—®é¢˜
    db_info: Optional[Dict]  # æ•°æ®åº“ä¿¡æ¯
    table_relationship: Optional[List[Dict[str, Any]]]  # è¡¨å…³ç³»
    generated_sql: Optional[str]  # ç”Ÿæˆçš„ SQL
    execution_result: Optional[ExecutionResult]  # SQL æ‰§è¡Œç»“æœ
    report_summary: Optional[str]  # æŠ¥å‘Šæ‘˜è¦
    attempts: int = 0  # å°è¯•æ¬¡æ•°
    chart_url: Optional[str]  # å›¾è¡¨åœ°å€
    chart_type: Optional[str]  # å›¾è¡¨ç±»å‹
    apache_chart_data: Optional[Dict[str, Any]]  # å›¾è¡¨æ•°æ®
```

**è®¾è®¡è¦ç‚¹:**
1. **ç±»å‹æ³¨è§£å®Œæ•´**: ä½¿ç”¨ `TypedDict` æä¾›ç±»å‹æç¤º
2. **Optional æ ‡æ³¨**: åˆå§‹çŠ¶æ€å¯èƒ½ä¸ºç©ºçš„å­—æ®µ
3. **ä¸šåŠ¡å»ºæ¨¡**: åŒ…å«å°è¯•æ¬¡æ•°ç­‰æ§åˆ¶å­—æ®µ

### 3.2 Node(èŠ‚ç‚¹):çŠ¶æ€è½¬æ¢å‡½æ•°

**èŠ‚ç‚¹çš„æœ¬è´¨:**
```python
# èŠ‚ç‚¹å°±æ˜¯ä¸€ä¸ªæ¥æ”¶ State å¹¶è¿”å› State çš„å‡½æ•°
def my_node(state: AgentState) -> AgentState:
    # 1. è¯»å–éœ€è¦çš„çŠ¶æ€
    user_query = state["user_query"]

    # 2. æ‰§è¡Œä¸šåŠ¡é€»è¾‘
    result = do_something(user_query)

    # 3. æ›´æ–°çŠ¶æ€
    state["some_field"] = result

    # 4. è¿”å›çŠ¶æ€
    return state
```

**é¡¹ç›®å®æˆ˜(`agent/text2sql/database/db_service.py:479`):**
```python
def get_table_schema(self, state: AgentState) -> AgentState:
    """
    æ ¹æ®ç”¨æˆ·æŸ¥è¯¢,é€šè¿‡æ··åˆæ£€ç´¢ç­›é€‰å‡ºæœ€ç›¸å…³çš„æ•°æ®åº“è¡¨ç»“æ„ã€‚

    Args:
        state (AgentState): å½“å‰çŠ¶æ€,åŒ…å« user_query

    Returns:
        AgentState: æ›´æ–°åçš„çŠ¶æ€,åŒ…å« db_info
    """
    try:
        logger.info("ğŸ” å¼€å§‹è·å–æ•°æ®åº“è¡¨ schema ä¿¡æ¯")
        all_table_info = self._fetch_all_table_info()

        user_query = state.get("user_query", "").strip()
        if not user_query:
            state["db_info"] = all_table_info
            return state

        # æ··åˆæ£€ç´¢:BM25 + å‘é‡æ£€ç´¢
        bm25_top_indices = self._retrieve_by_bm25(all_table_info, user_query)
        vector_top_indices = self._retrieve_by_vector(user_query, top_k=20)

        # RRF èåˆ
        fused_indices = self._rrf_fusion(bm25_top_indices, vector_top_indices)

        # é‡æ’åº
        candidate_tables = {self._table_names[i]: all_table_info[self._table_names[i]]
                           for i in fused_indices[:10]}
        reranked_results = self._rerank_with_dashscope(user_query, candidate_tables)

        # å– top 4
        final_table_names = [name for name, _ in reranked_results][:4]
        filtered_info = {name: all_table_info[name] for name in final_table_names}

        state["db_info"] = filtered_info
        logger.info(f"âœ… æœ€ç»ˆç­›é€‰å‡º {len(filtered_info)} ä¸ªç›¸å…³è¡¨: {list(filtered_info.keys())}")

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®åº“è¡¨ä¿¡æ¯å¤±è´¥: {e}")
        state["db_info"] = {}
        state["execution_result"] = ExecutionResult(success=False, error="æ— æ³•è¿æ¥æ•°æ®åº“")

    return state
```

**èŠ‚ç‚¹è®¾è®¡åŸåˆ™:**
1. **å•ä¸€èŒè´£**: æ¯ä¸ªèŠ‚ç‚¹åªåšä¸€ä»¶äº‹
2. **æ— å‰¯ä½œç”¨**: ä¸ä¿®æ”¹å¤–éƒ¨å˜é‡(åªä¿®æ”¹ state)
3. **å¼‚å¸¸å¤„ç†**: æ•è·å¼‚å¸¸å¹¶æ›´æ–°åˆ° state ä¸­
4. **æ—¥å¿—è®°å½•**: è®°å½•å…³é”®æ­¥éª¤ä¾¿äºè°ƒè¯•

### 3.3 Edge(è¾¹):èŠ‚ç‚¹é—´çš„è¿æ¥

**ä¸‰ç§è¾¹ç±»å‹:**

**1. æ™®é€šè¾¹(å›ºå®šè·¯ç”±):**
```python
graph.add_edge("node_a", "node_b")  # æ‰§è¡Œå®Œ node_a åæ€»æ˜¯æ‰§è¡Œ node_b
```

**2. æ¡ä»¶è¾¹(åŠ¨æ€è·¯ç”±):**
```python
def route_function(state: AgentState) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
    if state["execution_result"].success:
        return "summarize"
    else:
        return "error_handler"

graph.add_conditional_edges(
    "sql_executor",  # æºèŠ‚ç‚¹
    route_function,  # è·¯ç”±å‡½æ•°
    {
        "summarize": "summarize",  # è·¯ç”±å€¼ -> ç›®æ ‡èŠ‚ç‚¹
        "error_handler": "error_handler"
    }
)
```

**3. å…¥å£/å‡ºå£ç‚¹:**
```python
graph.set_entry_point("schema_inspector")  # èµ·å§‹èŠ‚ç‚¹
graph.add_edge("summarize", END)  # END æ˜¯ç‰¹æ®ŠèŠ‚ç‚¹,è¡¨ç¤ºæµç¨‹ç»“æŸ
```

**é¡¹ç›®å®æˆ˜(`agent/text2sql/analysis/graph.py:18`):**
```python
from langgraph.graph import StateGraph, END

def data_render_condition(state: AgentState) -> str:
    """
    æ ¹æ® chart_type åˆ¤æ–­ä½¿ç”¨å“ªç§å›¾è¡¨æ¸²æŸ“æ–¹å¼
    """
    chart_type = state.get("chart_type")
    if not chart_type or chart_type.lower() in ["mcp-server-chart-generate_table"]:
        return "data_render_apache"  # è¡¨æ ¼ä½¿ç”¨ Apache ECharts
    return "data_render"  # å…¶ä»–å›¾è¡¨ä½¿ç”¨ AntV

def create_graph():
    graph = StateGraph(AgentState)
    db_service = DatabaseService()

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("schema_inspector", db_service.get_table_schema)
    graph.add_node("sql_generator", sql_generate)
    graph.add_node("sql_executor", db_service.execute_sql)
    graph.add_node("data_render", data_render_ant)
    graph.add_node("data_render_apache", data_render_apache)
    graph.add_node("summarize", summarize)

    # æ·»åŠ è¾¹
    graph.set_entry_point("schema_inspector")

    # æ¡ä»¶è¾¹:æ˜¯å¦å¯ç”¨ Neo4j
    neo4j_enabled = os.getenv("NEO4J_ENABLED", "false").lower() == "true"
    if neo4j_enabled:
        graph.add_node("table_relationship", get_table_relationship)
        graph.add_edge("schema_inspector", "table_relationship")
        graph.add_edge("table_relationship", "sql_generator")
    else:
        graph.add_edge("schema_inspector", "sql_generator")

    graph.add_edge("sql_generator", "sql_executor")
    graph.add_edge("sql_executor", "summarize")

    # æ¡ä»¶è¾¹:æ ¹æ®å›¾è¡¨ç±»å‹é€‰æ‹©æ¸²æŸ“æ–¹å¼
    graph.add_conditional_edges(
        "summarize",
        data_render_condition,
        {
            "data_render": "data_render",
            "data_render_apache": "data_render_apache"
        }
    )

    graph.add_edge("data_render", END)
    graph.add_edge("data_render_apache", END)

    return graph.compile()
```

**æµç¨‹å›¾å¯è§†åŒ–:**
```
schema_inspector
       â†“
[Neo4jå¯ç”¨?]
   â†“      â†“
table_relationship  (è·³è¿‡)
       â†“      â†“
   sql_generator
       â†“
   sql_executor
       â†“
    summarize
       â†“
   [å›¾è¡¨ç±»å‹?]
   â†“         â†“
data_render  data_render_apache
   â†“         â†“
   END      END
```

### 3.4 Graph(å›¾):ç»„è£…å®Œæ•´æµç¨‹

**ç¼–è¯‘ä¸æ‰§è¡Œ:**
```python
from langgraph.graph.state import CompiledStateGraph

# åˆ›å»ºå›¾
graph = StateGraph(AgentState)
graph.add_node("step1", node1_func)
graph.add_node("step2", node2_func)
graph.add_edge("step1", "step2")
graph.set_entry_point("step1")
graph.add_edge("step2", END)

# ç¼–è¯‘(ä¼˜åŒ–æ‰§è¡Œè·¯å¾„)
compiled_graph: CompiledStateGraph = graph.compile()

# æ‰§è¡Œ
initial_state = AgentState(user_query="æŸ¥è¯¢è®¢å•æ•°æ®")
result = await compiled_graph.ainvoke(initial_state)
print(result["report_summary"])
```

**æµå¼æ‰§è¡Œ(å®æ—¶è·å–ä¸­é—´ç»“æœ):**
```python
async for chunk_dict in graph.astream(initial_state, stream_mode="updates"):
    node_name, node_output = next(iter(chunk_dict.items()))
    print(f"èŠ‚ç‚¹ {node_name} è¾“å‡º: {node_output}")
```

**é¡¹ç›®å®æˆ˜(`agent/text2sql/text2_sql_agent.py:55`):**
```python
async def run_agent(self, query: str, response=None, chat_id: str = None,
                    uuid_str: str = None, user_token=None) -> None:
    """è¿è¡Œ Text2SQL æ™ºèƒ½ä½“"""
    t02_answer_data = []
    t04_answer_data = {}
    current_step = None

    try:
        initial_state = AgentState(user_query=query, attempts=0, correct_attempts=0)
        graph: CompiledStateGraph = create_graph()

        # æµå¼æ‰§è¡Œ
        async for chunk_dict in graph.astream(initial_state, stream_mode="updates"):
            langgraph_step, step_value = next(iter(chunk_dict.items()))

            # å¤„ç†æ­¥éª¤å˜æ›´
            current_step = await self._handle_step_change(
                response, current_step, langgraph_step, t02_answer_data
            )

            # å¤„ç†å…·ä½“æ­¥éª¤å†…å®¹
            if step_value:
                await self._process_step_content(
                    response, langgraph_step, step_value,
                    t02_answer_data, t04_answer_data
                )
    except Exception as e:
        logger.error(f"Error in run_agent: {str(e)}")
```

**æµå¼è¾“å‡ºå¤„ç†:**
```python
async def _handle_step_change(self, response, current_step, new_step, t02_answer_data):
    """å¤„ç†æ­¥éª¤å˜æ›´æ—¶çš„UIæ˜¾ç¤º"""
    if self.show_thinking_process:
        if new_step != current_step:
            # å…³é—­å‰ä¸€ä¸ªæ­¥éª¤çš„æŠ˜å æ¡†
            if current_step and current_step not in ["summarize", "data_render"]:
                await self._close_current_step(response, t02_answer_data)

            # æ‰“å¼€æ–°æ­¥éª¤çš„æŠ˜å æ¡†
            if new_step not in ["summarize", "data_render"]:
                think_html = f"""<details style="color:gray;">
                             <summary>{new_step}...</summary>"""
                await self._send_response(response, think_html)
                t02_answer_data.append(think_html)

    return new_step, t02_answer_data
```

---

## å››ã€å¯¹æ¯”æ€»ç»“:ä¼ ç»Ÿæ–¹å¼ vs LangGraph

### 4.1 ä»£ç å¤æ‚åº¦å¯¹æ¯”

**ä¼ ç»Ÿæ–¹å¼:**
```python
# 70 è¡Œä»£ç ,åµŒå¥— 5 å±‚
async def traditional_pipeline(user_query):
    try:
        db_info = await get_db_info(user_query)
        if not db_info:
            return error_response("æœªæ‰¾åˆ°è¡¨")

        try:
            sql = await generate_sql(user_query, db_info)
            if not sql:
                return error_response("SQLç”Ÿæˆå¤±è´¥")

            try:
                result = await execute_sql(sql)
                if not result.success:
                    try:
                        corrected_sql = await correct_sql(sql, result.error)
                        result = await execute_sql(corrected_sql)
                        if not result.success:
                            return error_response("SQLæ‰§è¡Œå¤±è´¥")
                    except Exception as e:
                        return error_response(str(e))

                summary = await summarize(result.data)
                if needs_chart(user_query):
                    chart = await generate_chart(result.data)
                    return {"summary": summary, "chart": chart}
                return {"summary": summary}
            except Exception as e:
                return error_response(str(e))
        except Exception as e:
            return error_response(str(e))
    except Exception as e:
        return error_response(str(e))
```

**LangGraph æ–¹å¼:**
```python
# 30 è¡Œä»£ç ,æ¸…æ™°æ‰å¹³
def create_graph():
    graph = StateGraph(AgentState)

    graph.add_node("get_db_info", get_db_info)
    graph.add_node("generate_sql", generate_sql)
    graph.add_node("execute_sql", execute_sql)
    graph.add_node("correct_sql", correct_sql)
    graph.add_node("summarize", summarize)
    graph.add_node("generate_chart", generate_chart)

    graph.set_entry_point("get_db_info")
    graph.add_edge("get_db_info", "generate_sql")
    graph.add_edge("generate_sql", "execute_sql")

    graph.add_conditional_edges(
        "execute_sql",
        lambda state: "correct_sql" if not state["execution_result"].success else "summarize"
    )
    graph.add_edge("correct_sql", "execute_sql")  # é‡è¯•

    graph.add_conditional_edges(
        "summarize",
        lambda state: "generate_chart" if needs_chart(state["user_query"]) else END
    )

    return graph.compile()
```

### 4.2 å¯ç»´æŠ¤æ€§å¯¹æ¯”

| å¯¹æ¯”ç»´åº¦ | ä¼ ç»Ÿæ–¹å¼ | LangGraph |
|---------|---------|-----------|
| æµç¨‹å¯è§†åŒ– | âŒ éœ€è¦ç”»æµç¨‹å›¾ | âœ… è‡ªåŠ¨ç”Ÿæˆ Mermaid å›¾ |
| å•å…ƒæµ‹è¯• | âŒ éš¾ä»¥éš”ç¦»æµ‹è¯• | âœ… æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹æµ‹è¯• |
| é”™è¯¯å®šä½ | âŒ éœ€è¦é€å±‚è°ƒè¯• | âœ… å¯ä»¥çœ‹åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€å˜åŒ– |
| æµç¨‹è°ƒæ•´ | âŒ éœ€è¦ä¿®æ”¹å¤šå¤„ if-else | âœ… åªéœ€è°ƒæ•´è¾¹çš„è¿æ¥ |
| å¹¶è¡Œæ‰§è¡Œ | âŒ éœ€è¦æ‰‹åŠ¨ç®¡ç† asyncio.gather | âœ… è‡ªåŠ¨è¯†åˆ«å¯å¹¶è¡ŒèŠ‚ç‚¹ |

### 4.3 é€‚ç”¨åœºæ™¯

**ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼çš„åœºæ™¯:**
- æµç¨‹ç®€å•(3æ­¥ä»¥å†…)
- é€»è¾‘å›ºå®š,ä¸éœ€è¦åŠ¨æ€è·¯ç”±
- æ€§èƒ½è¦æ±‚æé«˜(LangGraph æœ‰å°‘é‡å¼€é”€)

**ä½¿ç”¨ LangGraph çš„åœºæ™¯:**
- å¤šæ­¥éª¤æµç¨‹(5æ­¥ä»¥ä¸Š)
- éœ€è¦æ¡ä»¶åˆ†æ”¯/å¾ªç¯
- éœ€è¦å®æ—¶å±•ç¤ºæ‰§è¡Œè¿›åº¦
- å›¢é˜Ÿåä½œ(ä¸åŒäººè´Ÿè´£ä¸åŒèŠ‚ç‚¹)

---

## äº”ã€é¡¹ç›®æ¶æ„æ€»è§ˆ

### 5.1 ä¸‰ä¸ªæ™ºèƒ½ä½“å¯¹æ¯”

| æ™ºèƒ½ä½“ | ç”¨é€” | æŠ€æœ¯æ ˆ | å¤æ‚åº¦ |
|-------|------|--------|--------|
| CommonReact | é€šç”¨å¯¹è¯é—®ç­” | LangGraph + MCP Tools | â­â­ |
| Text2SQL | æ•°æ®åº“é—®ç­” | LangGraph + BM25 + FAISS | â­â­â­â­ |
| Excel | Excelæ–‡ä»¶åˆ†æ | LangGraph + DuckDB | â­â­â­ |

### 5.2 CommonReact Agent æµç¨‹

```
ç”¨æˆ·è¾“å…¥
   â†“
[éœ€è¦è°ƒç”¨å·¥å…·?]
   â†“         â†“
è°ƒç”¨MCPå·¥å…·   ç›´æ¥å›ç­”
   â†“         â†“
   LLM ç”Ÿæˆå›ç­”
   â†“
è¿”å›ç»“æœ
```

### 5.3 Text2SQL Agent æµç¨‹

```
ç”¨æˆ·æŸ¥è¯¢
   â†“
æ£€ç´¢è¡¨ç»“æ„(BM25+å‘é‡+é‡æ’)
   â†“
[å¯ç”¨Neo4j?]
   â†“      â†“
è·å–è¡¨å…³ç³»  (è·³è¿‡)
   â†“      â†“
ç”ŸæˆSQL(Prompt Engineering)
   â†“
æ‰§è¡ŒSQL
   â†“
LLMæ€»ç»“ç»“æœ
   â†“
[éœ€è¦å›¾è¡¨?]
   â†“      â†“
ç”Ÿæˆå›¾è¡¨  è¿”å›æ–‡æœ¬
   â†“      â†“
è¿”å›ç»“æœ
```

---

## å…­ã€å®æˆ˜ç»ƒä¹ :æ„å»ºä¸€ä¸ªç®€å•çš„ LangGraph åº”ç”¨

### 6.1 éœ€æ±‚:æ•°å­¦è®¡ç®—æ™ºèƒ½ä½“

**åŠŸèƒ½:**
1. æ¥æ”¶ç”¨æˆ·çš„æ•°å­¦é—®é¢˜(å¦‚"è®¡ç®— 123 + 456")
2. åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨è®¡ç®—å™¨å·¥å…·
3. å¦‚æœéœ€è¦,è°ƒç”¨å·¥å…·å¹¶è¿”å›ç»“æœ
4. å¦åˆ™ç›´æ¥ç”¨ LLM å›ç­”

### 6.2 ä»£ç å®ç°

```python
from typing import TypedDict, Optional, Literal
from langgraph.graph import StateGraph, END
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# 1. å®šä¹‰çŠ¶æ€
class MathAgentState(TypedDict):
    user_query: str
    needs_calculator: bool
    calculation_result: Optional[float]
    final_answer: str

# 2. å®šä¹‰èŠ‚ç‚¹
def classify_question(state: MathAgentState) -> MathAgentState:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—å™¨"""
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template(
        "è¿™ä¸ªé—®é¢˜æ˜¯å¦éœ€è¦è®¡ç®—å™¨?åªå›ç­”'æ˜¯'æˆ–'å¦':{question}"
    )
    chain = prompt | llm
    response = chain.invoke({"question": state["user_query"]})
    state["needs_calculator"] = "æ˜¯" in response.content
    return state

def use_calculator(state: MathAgentState) -> MathAgentState:
    """è°ƒç”¨è®¡ç®—å™¨å·¥å…·"""
    # ç®€åŒ–ç¤ºä¾‹:ç”¨ eval è®¡ç®—(ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨å®‰å…¨çš„è®¡ç®—åº“)
    try:
        result = eval(state["user_query"].replace("è®¡ç®—", ""))
        state["calculation_result"] = result
    except:
        state["calculation_result"] = None
    return state

def generate_answer(state: MathAgentState) -> MathAgentState:
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    if state.get("calculation_result") is not None:
        state["final_answer"] = f"è®¡ç®—ç»“æœæ˜¯:{state['calculation_result']}"
    else:
        llm = ChatOpenAI()
        prompt = ChatPromptTemplate.from_template("è¯·å›ç­”:{question}")
        chain = prompt | llm
        response = chain.invoke({"question": state["user_query"]})
        state["final_answer"] = response.content
    return state

# 3. æ„å»ºå›¾
def create_math_agent():
    graph = StateGraph(MathAgentState)

    graph.add_node("classify", classify_question)
    graph.add_node("calculate", use_calculator)
    graph.add_node("answer", generate_answer)

    graph.set_entry_point("classify")

    graph.add_conditional_edges(
        "classify",
        lambda state: "calculate" if state["needs_calculator"] else "answer"
    )

    graph.add_edge("calculate", "answer")
    graph.add_edge("answer", END)

    return graph.compile()

# 4. ä½¿ç”¨
agent = create_math_agent()
result = agent.invoke({"user_query": "è®¡ç®— 123 + 456"})
print(result["final_answer"])  # è¾“å‡º: è®¡ç®—ç»“æœæ˜¯:579
```

---

## ä¸ƒã€æœ¬ç« æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **LangChain ä¸‰å¤§ç»„ä»¶**: Prompt + LLM + OutputParser
2. **LCEL é“¾å¼è°ƒç”¨**: é€šè¿‡ `|` ç»„åˆå¤šä¸ªç»„ä»¶
3. **LangGraph å››è¦ç´ **: State, Node, Edge, Graph
4. **çŠ¶æ€é©±åŠ¨**: æ‰€æœ‰æ•°æ®é€šè¿‡ State ä¼ é€’,èŠ‚ç‚¹æ— å‰¯ä½œç”¨
5. **å¯è§†åŒ–ä¼˜åŠ¿**: è‡ªåŠ¨ç”Ÿæˆæµç¨‹å›¾,ä¾¿äºç†è§£å’Œè°ƒè¯•

### 7.2 è®¾è®¡åŸåˆ™

1. **å•ä¸€èŒè´£**: æ¯ä¸ªèŠ‚ç‚¹åªåšä¸€ä»¶äº‹
2. **ç±»å‹å®‰å…¨**: ä½¿ç”¨ TypedDict å®šä¹‰çŠ¶æ€
3. **å¼‚å¸¸éš”ç¦»**: æ¯ä¸ªèŠ‚ç‚¹å†…éƒ¨å¤„ç†å¼‚å¸¸,ä¸å½±å“å…¨å±€
4. **æ—¥å¿—å®Œæ•´**: è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º

### 7.3 ä¸‹ä¸€ç« é¢„å‘Š

ä¸‹ä¸€ç« æˆ‘ä»¬å°†å®æˆ˜ **CommonReact Agent**,å­¦ä¹ å¦‚ä½•:
- é›†æˆ MCP(Model Context Protocol) å·¥å…·
- å®ç°å¤šè½®å¯¹è¯è®°å¿†
- å¤„ç†æµå¼è¾“å‡º
- ä»»åŠ¡å–æ¶ˆæœºåˆ¶

---

**å®Œæ•´æ–‡ä»¶æ¸…å•:**
- `agent/common_react_agent.py` (362 è¡Œ) - CommonReact Agent å®ç°
- `agent/text2sql/text2_sql_agent.py` (276 è¡Œ) - Text2SQL Agent ä¸»é€»è¾‘
- `agent/text2sql/state/agent_state.py` (68 è¡Œ) - çŠ¶æ€å®šä¹‰
- `agent/text2sql/analysis/graph.py` (65 è¡Œ) - å›¾æ„å»º
- `agent/text2sql/sql/generator.py` (104 è¡Œ) - SQLç”ŸæˆèŠ‚ç‚¹
- `agent/text2sql/analysis/llm_summarizer.py` (132 è¡Œ) - æ€»ç»“èŠ‚ç‚¹
